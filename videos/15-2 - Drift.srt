1
00:00:04,700 --> 00:00:12,410
This video I want to introduce to concepts you need to be on the lookout for when you're trying to do models and particularly in production abound.

2
00:00:12,410 --> 00:00:17,990
Caught the drift of your data and your concepts. Learning outcomes are few to know critical assumptions of machine learning,

3
00:00:17,990 --> 00:00:23,390
evaluation and deployment and understand how models can direct degrade over time.

4
00:00:23,390 --> 00:00:30,980
So a fundamental assumption that our machine learning evaluation makes or that machine learning makes.

5
00:00:30,980 --> 00:00:35,690
Excuse me, is that the test data?

6
00:00:35,690 --> 00:00:42,590
Looks like the training data. The test of specifically the test of the training data are drawn from the same distribution.

7
00:00:42,590 --> 00:00:46,910
And one of the really crucial parts of this is that the conditional probability.

8
00:00:46,910 --> 00:00:52,400
So the probability of y given a particular set of covariates or features X is the same.

9
00:00:52,400 --> 00:00:57,410
Between the two. Otherwise you couldn't do machine learning because the idea is to learn that conditional probability.

10
00:00:57,410 --> 00:01:02,600
Remember that we're trying to do estimates of just conditional expectation or estimates of conditional probability.

11
00:01:02,600 --> 00:01:06,270
If it's not the same and the training gave it to the test data, then you can't learn.

12
00:01:06,270 --> 00:01:15,390
The training data to predict the test data. But also, if the probability of Y is not consistent, then it's we're gonna be looking at different data.

13
00:01:15,390 --> 00:01:17,790
The class weights may be wrong.

14
00:01:17,790 --> 00:01:23,430
If the probability of the covariates are different, effectively, we're learning from data that's distributed differently.

15
00:01:23,430 --> 00:01:27,840
It might work, but it very well may not. We have this fundamental assumption.

16
00:01:27,840 --> 00:01:34,890
If you're trying to learn from Trinny data to predict to predict test data, you're assuming the test data looks like the training data.

17
00:01:34,890 --> 00:01:39,570
There's also then another assumption when you're trying to when your purpose of your talent testing

18
00:01:39,570 --> 00:01:44,310
your model is to determine if the model is going to work well when you actually go try to deploy it.

19
00:01:44,310 --> 00:01:54,060
So if you're trying to test a spam filters, accuracy you what your goal is not to build a better spam filter that works well on a test dataset.

20
00:01:54,060 --> 00:01:59,730
The goal is to build a spam filter that's going to correctly classify spams as they come into your system.

21
00:01:59,730 --> 00:02:10,320
And so they're we're making the crucial assumption that the probability of data, it's actually going to see the live data that the live data.

22
00:02:10,320 --> 00:02:19,690
Strong from the same distribution as our test data. Because if it's not, then what we've tested it on isn't what it's actually going to run on.

23
00:02:19,690 --> 00:02:24,430
And so we haven't actually evaluated the model for the perforates task,

24
00:02:24,430 --> 00:02:29,710
if the the actual data is going to see come from a different distribution than the data that we're testing on.

25
00:02:29,710 --> 00:02:34,360
So a few different ways that this can drift. So the crap class private lens can change.

26
00:02:34,360 --> 00:02:38,720
It might just be that spams become more likely. The feature distributions can change.

27
00:02:38,720 --> 00:02:45,880
So emails start using Pertti, messages start using more of a particular kind of text or the geographic

28
00:02:45,880 --> 00:02:51,190
distribution of the sender changes or other features that you're using changes, but also the relationship changes.

29
00:02:51,190 --> 00:02:59,360
What, like the way people write spams may change. And so then how you predict a spam from the features changes.

30
00:02:59,360 --> 00:03:04,300
One particular example. So examples like December purchasing is different.

31
00:03:04,300 --> 00:03:08,110
A model that is trained on January through November.

32
00:03:08,110 --> 00:03:11,830
You try to predict December, December looks different because the holiday season.

33
00:03:11,830 --> 00:03:15,970
Also, if you've got a model that was working great, you had seasonal effects, you had cyclical effects.

34
00:03:15,970 --> 00:03:24,590
You are doing really good at forecasting. Your sales revenues for four years.

35
00:03:24,590 --> 00:03:31,920
Kofod has so fundamentally changed how we go about purchasing both in person and online,

36
00:03:31,920 --> 00:03:38,470
that you have a massive distribution shift in your model is no longer gonna be valid.

37
00:03:38,470 --> 00:03:44,860
So how do you deal with this? I'm going to talk briefly about two ways to start trying to deal with it.

38
00:03:44,860 --> 00:03:49,690
The first thing zero to do is to be aware of the proof of the assumptions that

39
00:03:49,690 --> 00:03:53,320
you're making and the potential problems of a violation of those assumptions.

40
00:03:53,320 --> 00:04:02,020
You even noted, go look for them. But then off line, one thing we can do is we can temporal if our data have timestamps, we can temporarily split it,

41
00:04:02,020 --> 00:04:10,090
because in production, your goal isn't to predict a randomly selected set of data points from the other data points.

42
00:04:10,090 --> 00:04:13,840
Your goal is to predict the next data point from historical data points.

43
00:04:13,840 --> 00:04:20,630
So what you can do is you can simulate that. You can say. You're trying to predict sales.

44
00:04:20,630 --> 00:04:26,690
You can set as your test data November and use trading data everything before November.

45
00:04:26,690 --> 00:04:30,800
You'll want to use the Sambor because you don't want your if you like your model to look into the future.

46
00:04:30,800 --> 00:04:37,530
You're violating the simulation stuff. Fidelity. And but this is the basic idea.

47
00:04:37,530 --> 00:04:42,470
You train on the data before the test data and you test on on that day.

48
00:04:42,470 --> 00:04:48,300
You need to hyper print are tuning. You can test on. You can use the month before as tuning data or because of seasonal effects.

49
00:04:48,300 --> 00:04:55,050
You may want to use the same month, the year before. As for your hyper parameter tuning, but this is the idea.

50
00:04:55,050 --> 00:05:00,630
You do this temporal split upside, you get this increased fidelity of your what you're actually trying to soft new salt.

51
00:05:00,630 --> 00:05:01,950
Can you predict next data?

52
00:05:01,950 --> 00:05:08,450
Given the historical data, drawback is that we're not randomly sampling the results and so we have to be a lot more careful.

53
00:05:08,450 --> 00:05:16,480
But the statistical inference of the results. But. The trade off is often worth it for understanding problems that are going to be things that are

54
00:05:16,480 --> 00:05:23,110
going to be deployed in a temporal setting where we expect data distributions to change over time.

55
00:05:23,110 --> 00:05:26,590
Then when you go online, need to continuously monitor what your system is doing.

56
00:05:26,590 --> 00:05:34,450
If you have the ability to measure its online accuracy, its making its predictions of whether incoming messages on your platform are spam.

57
00:05:34,450 --> 00:05:37,120
And you can test whether that was right or not. Watch that.

58
00:05:37,120 --> 00:05:43,840
But even just watching other metrics, like the frequency with which it's classifying something as spam.

59
00:05:43,840 --> 00:05:51,820
If that jumps or if that drops. That doesn't necessarily mean your model performance is different, but it means something's changed.

60
00:05:51,820 --> 00:05:58,540
So if. Fifty percent of your messages are spam on usually.

61
00:05:58,540 --> 00:06:05,730
Or the spam reporter spam classifier flags 50 percent of that spam. And suddenly it starts only flacking 40 percent of spam.

62
00:06:05,730 --> 00:06:09,130
You should look into. That's a signal that you need to look into. Why?

63
00:06:09,130 --> 00:06:15,690
To see if you've got a data set shift, a status shift that's causing your model to no longer perform properly.

64
00:06:15,690 --> 00:06:19,140
Also, you need to regularly retrain and reevaluate your model.

65
00:06:19,140 --> 00:06:23,160
You can't just say, oh, I've got a model. It's got this accuracy. Good. Let's put it production.

66
00:06:23,160 --> 00:06:28,110
Let's run it for two years. You need to retrain the model frind with new data.

67
00:06:28,110 --> 00:06:32,910
And then also, you need to periodically reevaluate your model to see is it still giving the accuracy.

68
00:06:32,910 --> 00:06:40,850
So we're running it for a year, collecting more data. If we tested out some new data, is it still giving the accuracy that that we expect for it?

69
00:06:40,850 --> 00:06:48,270
So to wrap up. Machine learning, training and evaluation assumes that the training data and test data match each other and match real life.

70
00:06:48,270 --> 00:06:55,260
You can't always rely on that. You need to pay attention to shifts in your data and in its distributions.

71
00:06:55,260 --> 00:07:01,230
That makes your that either cause the model not to be able to learn from training to predict test data,

72
00:07:01,230 --> 00:07:13,934
or that cause the test data to no longer be an accurate assessment of what's going to happen when you try to use your model for real use.

