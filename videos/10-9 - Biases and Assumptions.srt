1
00:00:04,450 --> 00:00:05,410
Hello again.

2
00:00:05,410 --> 00:00:12,200
This video, I want to talk some more about biases we've been talking about a little bit throughout the class and also the assumptions that we make.

3
00:00:12,200 --> 00:00:15,080
We're doing a predictive modeling task.

4
00:00:15,080 --> 00:00:20,870
Learning outcomes are for you to build a reason about its potential biases and classification inputs and outputs and

5
00:00:20,870 --> 00:00:26,630
also identify some cases or building a classifier or a predictor it applies to regression to is not appropriate.

6
00:00:26,630 --> 00:00:31,070
So I want to talk a little bit about the assumptions of most predictive modeling tasks.

7
00:00:31,070 --> 00:00:34,880
So we make use that we have an outcome variable and observed I'm biased.

8
00:00:34,880 --> 00:00:42,560
That's not the same as not erroneous. But we have. But we assume that there's no systematic bias in our outcome variable or in our features that

9
00:00:42,560 --> 00:00:46,790
are outcome variable actually matches the target of the thing that we're trying to classify.

10
00:00:46,790 --> 00:00:50,390
And that predicting this outcome with these features is reasonable.

11
00:00:50,390 --> 00:00:57,790
Think about what you need to think about what the fact that you are trying to do this prediction implies.

12
00:00:57,790 --> 00:01:07,490
The one of the readings I've assigned is going to talk in more detail about the assumptions of using prediction systems to make decisions.

13
00:01:07,490 --> 00:01:17,570
So recall from week two, I talked about a few sources of bias. Selection bias is when your there's a there's a discrepancy in who gets selected.

14
00:01:17,570 --> 00:01:21,010
Your selection is not uniformly at random from the population.

15
00:01:21,010 --> 00:01:26,780
Some some instances are more likely to be selected than another that others response bias

16
00:01:26,780 --> 00:01:32,420
is that some some selected instances are more likely to give you a response than others.

17
00:01:32,420 --> 00:01:40,850
This is super common when we're dealing with human data, because even if you are perfectly unbiased and who you select to ask a survey question,

18
00:01:40,850 --> 00:01:44,630
people aren't necessarily all are people aren't always going to respond.

19
00:01:44,630 --> 00:01:56,360
And there may be a correlation between whether or not they respond and what their response would be if they responded.

20
00:01:56,360 --> 00:02:04,580
One one example of this. That's not a Soviet or refusing to respond is if you ask somebody to tell you they're to

21
00:02:04,580 --> 00:02:09,260
rate a random movie that they've never seen and you're not having them watch it just hey,

22
00:02:09,260 --> 00:02:17,620
what did you think of the 5000 fingers of Dr. T? They're more likely to be able to answer that question for movies they watched and

23
00:02:17,620 --> 00:02:22,180
they're more likely to have watched movies that they think they're going to like.

24
00:02:22,180 --> 00:02:29,530
And so you can think of this as a selection bias that users selecting movies to rate are more likely to psych movies they want to watch.

25
00:02:29,530 --> 00:02:37,280
But if you flip it around, so it's you asking a person, what do you think of the 5000 fingers of Dr. T?

26
00:02:37,280 --> 00:02:41,150
They're going to be more likely to respond if it's Poovey they thought they would have liked and watched.

27
00:02:41,150 --> 00:02:46,880
You're not going to get very many respondents on the 5000 fingers of Dr. T, and they measure my biases.

28
00:02:46,880 --> 00:02:52,340
You can get the response and it skews one way or another based in a systematic way,

29
00:02:52,340 --> 00:03:02,090
possibly based on on sensitive attributes, protected group classification of protected attributes of of the data subjects.

30
00:03:02,090 --> 00:03:07,840
So. It's important to notice that there's a difference between error and bias.

31
00:03:07,840 --> 00:03:13,720
Observations often have error in them. But the bias comes in when they are systematically erroneous.

32
00:03:13,720 --> 00:03:17,440
So when we talk about we talk about an unbiased estimate or statistics,

33
00:03:17,440 --> 00:03:23,200
an unbiased estimate or is an estimate or whose expected value is equal to the parameter or the expected value,

34
00:03:23,200 --> 00:03:31,000
the parameter a bias comes in when the the values the estimate is or the actual observations we're making are systematically higher,

35
00:03:31,000 --> 00:03:36,220
low, or they may trend different for different groups. And maybe the deal for all means is even.

36
00:03:36,220 --> 00:03:48,070
But one group tends to score higher on on your measurement than another group or or you're mis measuring one group more likely than another group.

37
00:03:48,070 --> 00:03:54,650
So if you have features or outcome variables that are unbiased, then you can just roll your errors into your model uncertainty.

38
00:03:54,650 --> 00:03:59,830
Everything's uncertain if the the errors are independent and identically distributed.

39
00:03:59,830 --> 00:04:04,240
It's more uncertainty in your model. If they have no bias, you may be able to correct.

40
00:04:04,240 --> 00:04:06,070
You may be able to remove a bias term.

41
00:04:06,070 --> 00:04:12,520
You may be able to make some assumptions like saying, well, there's a difference in the score between these two groups.

42
00:04:12,520 --> 00:04:21,620
We're going. But we don't believe the groups are actually different, so we'll just normalize them within groups.

43
00:04:21,620 --> 00:04:24,950
Some of these things are what happens in some of the election forecasting.

44
00:04:24,950 --> 00:04:34,040
So the election forecasters, they they pool together polling data from a bunch of polling sources, along with other data that affects their forecast.

45
00:04:34,040 --> 00:04:38,000
And one of the things they have is they have a model of the bias of different polling houses.

46
00:04:38,000 --> 00:04:45,740
So some polling houses do their sampling strategy might might be more likely to contact or there

47
00:04:45,740 --> 00:04:50,480
might be a Republican leaning bias or a Democrat leaning bias in their sampling strategies.

48
00:04:50,480 --> 00:04:53,810
And they're polling results. You can tell that you can.

49
00:04:53,810 --> 00:05:01,060
Look, see this in some ways by their agreement with each other, also by their agreement and their historical agreement with election outcomes.

50
00:05:01,060 --> 00:05:09,640
If you are assuming that the House bias is relatively stable over time and but if you've got good data to estimate how it's biases,

51
00:05:09,640 --> 00:05:17,980
you can use those to adjust your polling average polling when you're pool pooling together multiple polling sources for election modeling.

52
00:05:17,980 --> 00:05:25,170
That's one example of a way where you where there's deliberately trying to de bias, where you've got an estimate of the bias.

53
00:05:25,170 --> 00:05:30,330
There's unknown bias, which this is really common. You need to start to think about how and how severe.

54
00:05:30,330 --> 00:05:35,490
And first, can you start to try to quantify the bias, but then also what are the downstream applications?

55
00:05:35,490 --> 00:05:46,710
This is one of the reasons why we always start with our goal and we move to our question because some biases will affect the question.

56
00:05:46,710 --> 00:05:53,790
And for that, the same bias may render some questions unanswerable and have negligible impact on other questions.

57
00:05:53,790 --> 00:06:01,980
So the the the problems that arise from bias in our data are not intrinsic, necessarily intrinsic to the data ourselves itself.

58
00:06:01,980 --> 00:06:07,500
They arise in combination of what we're going to do with it and that that needs to

59
00:06:07,500 --> 00:06:15,690
inform how we go about understanding the impact of a potential bias in the data.

60
00:06:15,690 --> 00:06:25,280
We also, though, we need to think about the assumptions we bring to our data and also some assumptions can sometimes help us get out of some problems.

61
00:06:25,280 --> 00:06:29,070
We need to document them and be clear about them, but assumptions can provide us some guidance.

62
00:06:29,070 --> 00:06:35,100
So, for example, if we find S.A.T. scores differ by socioeconomic status, that's relatively well established.

63
00:06:35,100 --> 00:06:42,180
But what's what causes this? What are these is more likely that poor students are intrinsically less academically

64
00:06:42,180 --> 00:06:49,530
capable or that poor students have had less less access to academic preparation.

65
00:06:49,530 --> 00:06:57,120
And that can be formal academic preparation, such as S.A.T. prep courses, a greater selection of college prep courses in high school.

66
00:06:57,120 --> 00:07:03,960
And it may be less formal, such as a greater selection of reading materials when they were in elementary school.

67
00:07:03,960 --> 00:07:06,930
It's relatively well established that good access to reading,

68
00:07:06,930 --> 00:07:15,030
which had access to a good quantity of reading materials for young children and engagement with reading,

69
00:07:15,030 --> 00:07:21,570
can really help them with educational outcomes down the road.

70
00:07:21,570 --> 00:07:25,080
There's one study that we cite in some of the research that I've been I've been involved

71
00:07:25,080 --> 00:07:33,630
with that found that if you engage children in what they called authentic literacy tasks,

72
00:07:33,630 --> 00:07:37,800
but reading things besides a text book,

73
00:07:37,800 --> 00:07:46,470
when they are in the first or second grade, then later on in junior high around the junior high age or just a little bit younger,

74
00:07:46,470 --> 00:07:54,450
those students have higher learning outcomes when in in various STEM tasks.

75
00:07:54,450 --> 00:07:59,130
And so that can be the kind of this informal a student who has early access to good

76
00:07:59,130 --> 00:08:04,620
reading materials and diverse reading materials is going to do better academically.

77
00:08:04,620 --> 00:08:09,540
This is a good, good reason to believe they'll do better on the S.A.T.

78
00:08:09,540 --> 00:08:15,900
And so one of my one one of the research projects I'm involved with is are working on

79
00:08:15,900 --> 00:08:24,160
is looking at how do we how can we use technology tools to enable to make it easier?

80
00:08:24,160 --> 00:08:33,600
For teachers to provide more and different reading materials for their students in a way with minimal costs,

81
00:08:33,600 --> 00:08:41,530
it's accessible in low resource educational settings. Then there's also the question of does the S.A.T. measure some combination of academic

82
00:08:41,530 --> 00:08:49,900
capability and familiarity with conventions and expectations for middle to upper class?

83
00:08:49,900 --> 00:08:58,400
So when you see a difference like this. Anything. What's more like is it really more likely that the students are intrinsically different or that?

84
00:08:58,400 --> 00:09:03,790
There's this difference in access to academic preparation. And that's what we're actually measuring.

85
00:09:03,790 --> 00:09:06,430
And if that's what we're actually measuring,

86
00:09:06,430 --> 00:09:13,150
what implications should that have for how we actually use the resulting numbers and what we do with them?

87
00:09:13,150 --> 00:09:16,990
So I also want to talk a little bit about outcome target mismatch.

88
00:09:16,990 --> 00:09:25,810
So what I mean by this is you got a Klatt, you're trying to predict X and you have a class label for X Prime.

89
00:09:25,810 --> 00:09:34,360
That's not really X. And so you're actually training a model to predict one thing when your goal is another.

90
00:09:34,360 --> 00:09:38,340
Sometimes you have to. Sometimes all we have is a proxy. And that's a reasonable thing to do.

91
00:09:38,340 --> 00:09:46,580
We need to evaluate the quality and the credibility of our proxy because sometimes it's all we have.

92
00:09:46,580 --> 00:09:52,280
But sometimes the proxy is too disconnected from the target to be credible.

93
00:09:52,280 --> 00:10:00,770
For example, if we want to model crime. So if we want to classify neighborhoods, if we want to be able to predict the crime level of a neighborhood.

94
00:10:00,770 --> 00:10:05,830
And the data we have as crime reports and or arrests.

95
00:10:05,830 --> 00:10:14,290
What we're trained to predict or to do is predict crime reports and police activity, which is not the same thing as actual crime,

96
00:10:14,290 --> 00:10:20,110
because an area where crimes go unreported, because there could be a variety of reasons.

97
00:10:20,110 --> 00:10:27,820
One could be that the people in that neighborhood have less trust for the police, with the police.

98
00:10:27,820 --> 00:10:35,830
And so they're less likely to report minor crimes or crimes aren't observed because that area isn't as heavily police.

99
00:10:35,830 --> 00:10:42,130
So crime is happening. But police aren't there to observe it and make arrests and nobody's bothering to report it.

100
00:10:42,130 --> 00:10:49,800
So if you're trying to predict crime, but your prediction model is trained on.

101
00:10:49,800 --> 00:10:56,820
Crime reports or police activity? You're not predicting crime. You're predicting crime reports or police activity.

102
00:10:56,820 --> 00:11:06,480
And so you have to be really careful about the relationship between the labels you have and the actual target variable that you're trying to measure.

103
00:11:06,480 --> 00:11:11,760
So with that, our claims then need to be supported by our evidence.

104
00:11:11,760 --> 00:11:21,390
And so we can't claim that we're detecting crime when we are trusting our classifier on crime reports because we're testing its ability to detect.

105
00:11:21,390 --> 00:11:26,040
They reported crime, which is not the same thing as a crime that actually happened.

106
00:11:26,040 --> 00:11:32,760
We're missing all of the unreported crimes and we also have the reports of things that weren't actually crimes.

107
00:11:32,760 --> 00:11:38,640
This is why early on in the semester, I talked with you about this goal question analysis chain.

108
00:11:38,640 --> 00:11:45,020
And I encourage you to go back and review that material because we need we have a goal.

109
00:11:45,020 --> 00:11:49,650
We would find that in the research questions and then we connect the research questions to the analysis.

110
00:11:49,650 --> 00:11:55,920
And at every step, it needs to be clear, the analysis we do needs to directly illuminate the research question and we need

111
00:11:55,920 --> 00:12:02,130
to revise the analysis and or reframe the research question until we have a match.

112
00:12:02,130 --> 00:12:10,950
That the analysis is actually addressing the question, can we detect crime reports?

113
00:12:10,950 --> 00:12:17,310
And then the question needs to advance our goal and breaks anywhere in that chain.

114
00:12:17,310 --> 00:12:23,650
Reduce the ability of our data analysis to advance the goals that we're trying to use it for.

115
00:12:23,650 --> 00:12:26,590
We also need to think about the reasonableness of the task,

116
00:12:26,590 --> 00:12:34,870
because whenever we use acts to predict why, we are assuming that that's a reasonable thing to do.

117
00:12:34,870 --> 00:12:39,270
Predicting college performance with S.A.T. scores. That's kind of what the S.A.T. is built for.

118
00:12:39,270 --> 00:12:40,870
There are problems. The S.A.T.,

119
00:12:40,870 --> 00:12:49,180
but this isn't an inherently unreasonable task performance on a standardized test as a legitimate basis for predicting future academic grades.

120
00:12:49,180 --> 00:12:54,610
On its face, not a bad assumption, but. Every year or so,

121
00:12:54,610 --> 00:13:00,580
somebody or another gets the idea that they're going to take a deep learner and they're going to train out of a bunch of photos

122
00:13:00,580 --> 00:13:08,140
and their outcome variable is going to be some attempted measure of criminality has been arrested or maybe has been convicted.

123
00:13:08,140 --> 00:13:15,430
And trying to convey trying to trying to predict criminality from photos and what

124
00:13:15,430 --> 00:13:19,870
this assumes is that facial features are a legitimate predictor of criminality.

125
00:13:19,870 --> 00:13:26,900
And so the question can arise, why would you assume that? This this mechanism is called physiognomy.

126
00:13:26,900 --> 00:13:33,980
It's been rejected for a good, solid century, it was kicking around in the eighteen hundreds and then people realized that it was a bad idea.

127
00:13:33,980 --> 00:13:40,290
It's close cousin. It's phrenology. Physiognomy is when you're trying to predict attributes such as criminality or other personality traits.

128
00:13:40,290 --> 00:13:45,680
My face fits. Phrenology is when you're trying to do it based on the shape of a skull.

129
00:13:45,680 --> 00:13:48,950
It involves a lot of calibers and school measurements.

130
00:13:48,950 --> 00:14:01,730
But you it's it's assuming here that you can use these physical characteristics to predict criminality and you can probably predict arrested.

131
00:14:01,730 --> 00:14:09,050
You can put you might even be able to predict charged or convicted with a crime because the social process of

132
00:14:09,050 --> 00:14:17,000
constructing what is a crime and who gets arrested for a crime is going to have correlates to physical attributes.

133
00:14:17,000 --> 00:14:26,960
But. At its base, we have to think about what is crime and crime is actions.

134
00:14:26,960 --> 00:14:36,260
That as a society, we have decided are sufficiently aberrant that they deserve criminal treatment.

135
00:14:36,260 --> 00:14:49,030
Some of those are relatively uncontroversial, like theft. But if crime is an action in violation of our societal laws.

136
00:14:49,030 --> 00:14:56,110
Why would that be a physically observable characteristic? What's the theory, what's the mechanism there?

137
00:14:56,110 --> 00:15:00,260
Because you can find all kinds of correlations and theory and thinking about

138
00:15:00,260 --> 00:15:06,880
as the assumptions and what theoretical constructs could motivate something.

139
00:15:06,880 --> 00:15:17,450
Are a key. Ah, one of the guiding points we can use to keep away from some of these madrassas.

140
00:15:17,450 --> 00:15:22,340
So be careful what you assume. Theory drives our research questions. We don't just ask every question willy nilly.

141
00:15:22,340 --> 00:15:25,280
That's a very inefficient use of science theory either.

142
00:15:25,280 --> 00:15:33,650
The theory we have that we want to clarify or that we want to apply to a new problem or propose theories that we want to evaluate,

143
00:15:33,650 --> 00:15:36,950
we should first give them a smell test and see it.

144
00:15:36,950 --> 00:15:43,340
Is this a reasonable theory to try to evaluate? And theory also drives or drives our predictions.

145
00:15:43,340 --> 00:15:49,370
We don't want to just throw a bunch of data at Saikat Learn or at Tenzer Flower or whatever and use whatever predictions come out.

146
00:15:49,370 --> 00:15:54,950
We want to put some thought into the process and we want to think about is this a reasonable prediction task?

147
00:15:54,950 --> 00:15:59,570
Is this a reasonable set of features? Is it legitimate to try to predict?

148
00:15:59,570 --> 00:16:08,630
Is this person a criminal? Based on what they look like, based on the picture that you see in the CCTV camera?

149
00:16:08,630 --> 00:16:18,370
Is that suppose, even suppose we did have a reliable correlation, is that a societally legitimate or useful thing to do?

150
00:16:18,370 --> 00:16:26,170
So another problem, though, that we also have is we can have problems with labeled dependency, so observations are often incomplete.

151
00:16:26,170 --> 00:16:32,530
For example, if the bank does not give someone a loan, they don't get to observe whether or not they're going to pay that loan back.

152
00:16:32,530 --> 00:16:39,410
This is a problem. Machine learning researchers, stative statistics, researchers like to give things clever names.

153
00:16:39,410 --> 00:16:49,480
This is a problem called the apple tasting problem. Also, though, criminal databases only have those were caught by the justice system in some way,

154
00:16:49,480 --> 00:16:56,710
it doesn't have the criminals that people who committed crimes and didn't get caught.

155
00:16:56,710 --> 00:17:07,330
And also, we have to be careful of inverse probabilities. So the probability of A given B and the probability of being given A are not the same thing.

156
00:17:07,330 --> 00:17:15,300
You have to be careful when you're using this to say if you've got two groups, say the group and you look try to look at their composition.

157
00:17:15,300 --> 00:17:21,290
That does not. That that that's not enough evidence to make.

158
00:17:21,290 --> 00:17:23,870
Conclusions in the other direction.

159
00:17:23,870 --> 00:17:34,250
So if you look at the racial makeup of basketball players, what you get is probability of race given basketball player.

160
00:17:34,250 --> 00:17:41,320
But that doesn't get give you a probability of skilled basketball player given race.

161
00:17:41,320 --> 00:17:43,720
And.

162
00:17:43,720 --> 00:17:52,760
So you have to be really careful about accidentally inverting your probability when you don't have the rest of the pieces of Bayes theorem involved.

163
00:17:52,760 --> 00:17:57,860
This is a this is one of the traps of common in formal probabilistic reasoning.

164
00:17:57,860 --> 00:18:01,310
You also have to be careful with pulling from different groups.

165
00:18:01,310 --> 00:18:09,200
So one example is like if if you're if you're if you have a bunch of mug shots and people getting arrested using those, you're criminal cases.

166
00:18:09,200 --> 00:18:12,520
Well, what are you using as a non-criminal face?

167
00:18:12,520 --> 00:18:17,980
If you're getting it from a different set, then are you really learning criminality or are you learning the visual distinct,

168
00:18:17,980 --> 00:18:22,410
the distinctive visual features of a mug shot? This also comes up in a variety of other settings.

169
00:18:22,410 --> 00:18:28,850
You need to you need to pay attention to what your learner is actually learning when it's going to try to do a prediction.

170
00:18:28,850 --> 00:18:36,980
I read of a case a few years ago where a machine learning algorithm for examining X-ray photos

171
00:18:36,980 --> 00:18:43,120
was trying to learn to to identify X-ray photos that indicated a particular medical condition.

172
00:18:43,120 --> 00:18:52,630
Had relatively good accuracy, but. Someone went and dug into what it was actually learning and see what parts of the images it looking at.

173
00:18:52,630 --> 00:19:05,580
And it was looking at over on the side, this code that indicated where the X-ray was taken because some of the X-ray pictures came from.

174
00:19:05,580 --> 00:19:12,900
A hospital where people were far more likely to have the disease.

175
00:19:12,900 --> 00:19:16,990
And the other one came for more general hospital or more general X-ray lab.

176
00:19:16,990 --> 00:19:21,940
And so it wasn't actually learning to identify the disease in the X-ray photo.

177
00:19:21,940 --> 00:19:27,310
It was learning the X-ray photos taken in a particular hospital's lab were more likely

178
00:19:27,310 --> 00:19:32,500
to have the disease because it was where the more advanced cases were being sent.

179
00:19:32,500 --> 00:19:36,010
So you have to be really, really careful, even if you've got a reasonable set of data.

180
00:19:36,010 --> 00:19:38,380
Oh, I have a bunch of X-ray photos.

181
00:19:38,380 --> 00:19:44,050
There can be differences that you don't expect that you need to be careful about what your systems actually learning.

182
00:19:44,050 --> 00:19:48,970
So what do you do about all of this? Unfortunately, there's not just a quick fix solution.

183
00:19:48,970 --> 00:19:54,320
You can't import psychic psychic lern dot unbias.

184
00:19:54,320 --> 00:19:59,310
And if someone ever gives us killer and that unbias be very, very skeptical.

185
00:19:59,310 --> 00:20:05,410
The starting point was we assume that we've got bias problems, we just don't necessarily know what they are, especially if you've got social data.

186
00:20:05,410 --> 00:20:09,670
The question isn't, are the data biased? The question is how are they biased? How much are they biased?

187
00:20:09,670 --> 00:20:15,980
And what impact does that have on the conclusions and tasks to which we're trying to apply it?

188
00:20:15,980 --> 00:20:21,380
We then need to start understanding how the data is collected and what the labels actually are.

189
00:20:21,380 --> 00:20:25,670
This is why we spent so much time early in the class talking about how do you actually describe data?

190
00:20:25,670 --> 00:20:32,060
And I have you read things about describing where your data came from, how it was collected, why it was collected,

191
00:20:32,060 --> 00:20:39,920
because incentive structures can skew the data collection process so that you have the information to start doing the reasoning about it.

192
00:20:39,920 --> 00:20:47,180
Study what's known about the data biases in your domain. There may be a plethora of radiate a body of existing research.

193
00:20:47,180 --> 00:20:50,810
You can draw off of to understand what's going on in it.

194
00:20:50,810 --> 00:20:56,400
Also look for systematic variations in the data, especially if you have data from different groups of people.

195
00:20:56,400 --> 00:20:59,390
If you have data from different sites, look for systematic variations.

196
00:20:59,390 --> 00:21:06,260
Those alone aren't necessarily aren't enough to tell you the drivers of different biases, but they give you a starting point where to go looking.

197
00:21:06,260 --> 00:21:10,910
And then you can go look for research like what might cause this kind of a difference between the groups that I'm seeing.

198
00:21:10,910 --> 00:21:17,160
Also, clarify a document, all of your assumptions. We want our analysis.

199
00:21:17,160 --> 00:21:20,760
We want to do a good job with our analysis. Our analysis will never be perfect.

200
00:21:20,760 --> 00:21:29,130
At some point they need to be done, but clarify and document what you're assuming each step of the pathway document why you're building this model.

201
00:21:29,130 --> 00:21:34,620
What's your theoretical justification for using these features to predict this outcome?

202
00:21:34,620 --> 00:21:41,930
What does that theoretical justification have to say about how you should use this features and how you should evaluate your model?

203
00:21:41,930 --> 00:21:47,720
Always be critical of of your own work and a reasonably critical the work of others as well.

204
00:21:47,720 --> 00:21:52,550
Does your problem make sense that your outcome makes sense? Are the results too good to be true?

205
00:21:52,550 --> 00:21:57,560
Too good to be true? They often are. But then also read broadly and critically.

206
00:21:57,560 --> 00:22:01,880
And this is one that it's hard to give a quick fix on, too.

207
00:22:01,880 --> 00:22:08,360
But a lot of what I learn about the way biases creep in and how to deal with them and the data that I work

208
00:22:08,360 --> 00:22:17,330
with and a lot of it's domain specific and contextual is from reading widely and reading deeply sometimes,

209
00:22:17,330 --> 00:22:20,840
but reading a lot of different things.

210
00:22:20,840 --> 00:22:28,610
And not just the statistics research, the data science research at the computer science research, but reading, pop science work.

211
00:22:28,610 --> 00:22:32,930
Good pop science work, reading, legal scholarship, reading,

212
00:22:32,930 --> 00:22:39,680
various other things to give you a more holistic picture of what is going on in the domains that I'm trying to study.

213
00:22:39,680 --> 00:22:45,360
So to wrap up all of our analysis are based on assumptions, and you need to be clear what your assumptions are.

214
00:22:45,360 --> 00:22:49,760
And you just study how your data is biased. And there's no magic bullet for all of this.

215
00:22:49,760 --> 00:22:55,580
We're gonna be talking about some things. We're going to talk about some measures for how do you measure bias and outcomes of a system.

216
00:22:55,580 --> 00:23:03,320
But there's no magic bullet. It requires continuous critical thought and reflection on what it is that we're doing and interrogation of what our

217
00:23:03,320 --> 00:23:13,730
system is doing and how its impacts are distributed and what its underlying data and conceptual theoretical bases are.

218
00:23:13,730 --> 00:23:20,570
This is also where the place where a few weeks ago we had the video about epistemology is this is one of the sources.

219
00:23:20,570 --> 00:23:29,930
This is one of the places where critical epistemology is can become very useful because they they give us the starting point for wit,

220
00:23:29,930 --> 00:23:36,110
the ways in which our system could go wrong and or might be a bad idea.

221
00:23:36,110 --> 00:23:39,710
That doesn't mean we thwe we just shut it down because someone said something.

222
00:23:39,710 --> 00:23:46,130
But it's something we need to reflect on it and incorporate what we learned from reading critical scholarship

223
00:23:46,130 --> 00:23:57,467
and reading critical analysis into how we think about going about the work that we're trying to do.

